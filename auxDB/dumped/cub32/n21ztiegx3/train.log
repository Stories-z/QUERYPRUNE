INFO - 10/28/19 14:19:56 - 0:00:00 - ============ Initialized logger ============
INFO - 10/28/19 14:19:56 - 0:00:00 - batch_size: 300
                                     category_number: 0
                                     category_sampler: False
                                     clip_grad_norm: 5
                                     code_size: 32
                                     command: python train.py --exp_name cub32 --batch_size 300 --continue_train True --load_model '../auxcnn/Best_model.pth --exp_id "n21ztiegx3"
                                     continue_train: True
                                     dataset_name: CUB_200_2011
                                     dump_path: ./dumped/cub32/n21ztiegx3
                                     epochs: 4000
                                     eval_only: False
                                     exp_id: n21ztiegx3
                                     exp_name: cub32
                                     gpu: 0
                                     load_model: ../auxcnn/Best_model.pth
                                     num_workers: 8
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.001
                                     root: ../datasets
                                     triplet_margin: 8
INFO - 10/28/19 14:19:56 - 0:00:00 - The experiment will be stored in ./dumped/cub32/n21ztiegx3
                                     
INFO - 10/28/19 14:19:56 - 0:00:00 - Running command: python train.py --exp_name cub32 --batch_size 300 --continue_train True --load_model '../auxcnn/Best_model.pth

INFO - 10/28/19 14:19:56 - 0:00:00 - Loading CUB_200_2011 dataset
INFO - 10/28/19 14:20:02 - 0:00:07 - Start Training
INFO - 10/28/19 14:20:02 - 0:00:07 - ============ Starting epoch 0 ... ============
INFO - 10/28/19 14:20:06 - 0:00:11 - Batch 1/20: loss: 48.380165
INFO - 10/28/19 14:20:07 - 0:00:11 - Batch 2/20: loss: 30.742554
INFO - 10/28/19 14:20:08 - 0:00:12 - Batch 3/20: loss: 25.166384
INFO - 10/28/19 14:20:09 - 0:00:13 - Batch 4/20: loss: 25.336420
INFO - 10/28/19 14:20:10 - 0:00:14 - Batch 5/20: loss: 22.366529
INFO - 10/28/19 14:20:11 - 0:00:15 - Batch 6/20: loss: 8.916077
INFO - 10/28/19 14:20:12 - 0:00:16 - Batch 7/20: loss: 17.017569
INFO - 10/28/19 14:20:13 - 0:00:17 - Batch 8/20: loss: 15.052497
INFO - 10/28/19 14:20:13 - 0:00:18 - Batch 9/20: loss: 30.682968
INFO - 10/28/19 14:20:14 - 0:00:19 - Batch 10/20: loss: 29.413773
INFO - 10/28/19 14:20:15 - 0:00:20 - Batch 11/20: loss: 9.752801
INFO - 10/28/19 14:20:16 - 0:00:20 - Batch 12/20: loss: 16.284233
INFO - 10/28/19 14:20:17 - 0:00:21 - Batch 13/20: loss: 11.754643
INFO - 10/28/19 14:20:18 - 0:00:22 - Batch 14/20: loss: 16.649815
INFO - 10/28/19 14:20:19 - 0:00:23 - Batch 15/20: loss: 5.979645
INFO - 10/28/19 14:20:20 - 0:00:24 - Batch 16/20: loss: 8.878378
INFO - 10/28/19 14:20:20 - 0:00:25 - Batch 17/20: loss: 32.111290
INFO - 10/28/19 14:20:21 - 0:00:26 - Batch 18/20: loss: 25.631142
INFO - 10/28/19 14:20:22 - 0:00:27 - Batch 19/20: loss: 39.210106
INFO - 10/28/19 14:20:23 - 0:00:28 - Batch 20/20: loss: 29.027905
INFO - 10/28/19 14:20:24 - 0:00:28 - ============ End of epoch 0 ============
